{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: Web Scraping con Selenium y Docker\n",
    "### Objetivo:\n",
    "\n",
    "Aprender a utilizar Selenium junto con Docker para realizar web scraping y extraer información de una página web. En esta tarea, extraerás los títulos de noticias de una página web de tu elección o la proporcionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de Web Scraping\n",
    "\n",
    "A continuación, te proporciono un script en Python utilizando Selenium para obtener los títulos de noticias de una página web. En este caso, utilizaremos la página de noticias de \"https://news.ycombinator.com/\" como ejemplo.\n",
    "\n",
    "Tu debes completar el script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Corre en modo headless para mayor eficiencia\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "# Abre la página de noticias (Hacker News en este caso)\n",
    "driver.get(\"https://news.ycombinator.com/\")\n",
    "\n",
    "# Extrae los títulos de las noticias\n",
    "titles = driver.find_elements(By.CLASS_NAME, \"titleline\")\n",
    "\n",
    "# Imprime los títulos\n",
    "for title in titles:\n",
    "    print(title.text)\n",
    "\n",
    "# Cierra el navegador\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del Código:\n",
    "\n",
    "* Configuración de Selenium: Se configura Selenium con el navegador Chrome utilizando opciones específicas para ejecutarlo dentro de Docker.\n",
    "\n",
    "* Acceso a la Página Web: El script se conecta a la página de noticias de \"YCombinator\" y carga el contenido.\n",
    "\n",
    "* Extracción de Datos: Se utiliza find_elements con la clase storylink para obtener todos los títulos de las noticias en la página.\n",
    "\n",
    "* Impresión de Resultados: Los títulos extraídos se imprimen en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "* Buscar dentro de la página web alguno de los links de arriba (New, Past, etc) y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome options for headless browsing\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Open the Hacker News main page\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "\n",
    "    # Find the \"Past\" link and click it\n",
    "    past_link = driver.find_element(By.LINK_TEXT, \"Past\")\n",
    "    past_link.click()\n",
    "\n",
    "    # Extract titles and their corresponding URLs from the \"Past\" section\n",
    "    titles = driver.find_elements(By.CSS_SELECTOR, \".titleline a\")\n",
    "\n",
    "    # Create a list to hold title and URL data\n",
    "    data = [{\"Title\": title.text, \"URL\": title.get_attribute(\"href\")} for title in titles]\n",
    "\n",
    "    # Convert the data into a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file (optional)\n",
    "    df.to_csv(\"past_section_titles.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3:\n",
    "\n",
    "* Buscar algo dentro la pagina en el apartado de \"Search\" y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Chrome options for headless browsing\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Open the Hacker News main page\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "\n",
    "    # Find the search bar (input with placeholder \"Search:\")\n",
    "    search_box = driver.find_element(By.CSS_SELECTOR, 'input[placeholder=\"Search\"]')\n",
    "\n",
    "    # Type \"dune\" into the search bar and press Enter\n",
    "    search_box.send_keys(\"dune\")\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Extract titles and their corresponding URLs from the search results\n",
    "    results = driver.find_elements(By.CSS_SELECTOR, \".titleline a\")\n",
    "\n",
    "    # Create a list to hold title and URL data\n",
    "    data = [{\"Title\": result.text, \"URL\": result.get_attribute(\"href\")} for result in results]\n",
    "\n",
    "    # Convert the data into a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file (optional)\n",
    "    df.to_csv(\"dune_search_results.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del Script:\n",
    "    \n",
    "Para ejecutar el script, asegúrate de que los contenedores de Docker estén corriendo y luego ejecuta el siguiente comando en la terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose run --rm python python script.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTREGABLES: \n",
    "* script.py: El archivo con el código para realizar el web scraping y extraer los títulos de las noticias.\n",
    "* README.md: Instrucciones para ejecutar el proyecto."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
