{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: Web Scraping con Selenium y Docker\n",
    "### Objetivo:\n",
    "\n",
    "Aprender a utilizar Selenium junto con Docker para realizar web scraping y extraer información de una página web. En esta tarea, extraerás los títulos de noticias de una página web de tu elección o la proporcionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de Web Scraping\n",
    "\n",
    "A continuación, te proporciono un script en Python utilizando Selenium para obtener los títulos de noticias de una página web. En este caso, utilizaremos la página de noticias de \"https://news.ycombinator.com/\" como ejemplo.\n",
    "\n",
    "Tu debes completar el script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://localhost:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "# Abre la página de noticias (aqui debes poner una pagina web de tu elección o la proporcionada)\n",
    "driver.get(\"https://news.ycombinator.com\")\n",
    "\n",
    "# Extrae los títulos de las noticias\n",
    "titles = driver.find_elements(By.CLASS_NAME, \"storylink\")\n",
    "\n",
    "# Imprime los títulos\n",
    "for title in titles:\n",
    "    print(title.text)\n",
    "\n",
    "# Cierra el navegador\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del Código:\n",
    "\n",
    "* Configuración de Selenium: Se configura Selenium con el navegador Chrome utilizando opciones específicas para ejecutarlo dentro de Docker.\n",
    "\n",
    "* Acceso a la Página Web: El script se conecta a la página de noticias de \"YCombinator\" y carga el contenido.\n",
    "\n",
    "* Extracción de Datos: Se utiliza find_elements con la clase storylink para obtener todos los títulos de las noticias en la página.\n",
    "\n",
    "* Impresión de Resultados: Los títulos extraídos se imprimen en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "* Buscar dentro de la página web alguno de los links de arriba (New, Past, etc) y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "# Abre la página de noticias\n",
    "driver.get(\"https://news.ycombinator.com\")\n",
    "\n",
    "# Extrae los enlaces de navegación en la parte superior (New, Past, etc.)\n",
    "nav_links = driver.find_elements(By.CSS_SELECTOR, \"a[href]\")  # Selecciona todos los enlaces con el atributo href\n",
    "\n",
    "# Filtra y organiza los enlaces\n",
    "links_data = []\n",
    "for link in nav_links:\n",
    "    text = link.text\n",
    "    href = link.get_attribute(\"href\")\n",
    "    if text:  # Asegúrate de capturar solo los enlaces con texto visible\n",
    "        links_data.append({\"Text\": text, \"URL\": href})\n",
    "\n",
    "# Crea un DataFrame con los enlaces\n",
    "df = pd.DataFrame(links_data)\n",
    "\n",
    "# Imprime el DataFrame\n",
    "print(df)\n",
    "\n",
    "# Cierra el navegador\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3:\n",
    "\n",
    "* Buscar algo dentro la pagina en el apartado de \"Search\" y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "# Abre la página de noticias\n",
    "driver.get(\"https://news.ycombinator.com\")\n",
    "\n",
    "# Busca el campo de búsqueda (puedes ajustar el selector dependiendo de la estructura de la página)\n",
    "search_box = driver.find_element(By.NAME, \"q\") \n",
    "\n",
    "# Escribe el texto que quieres buscar y presiona Enter\n",
    "search_query = \"Search\" \n",
    "search_box.send_keys(search_query)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Espera a que carguen los resultados (ajusta el tiempo según la página)\n",
    "time.sleep(3)\n",
    "\n",
    "# Extrae los resultados de la búsqueda\n",
    "results = driver.find_elements(By.CSS_SELECTOR, \"a.storylink\") \n",
    "\n",
    "# Organiza los resultados en una lista\n",
    "results_data = []\n",
    "for result in results:\n",
    "    title = result.text\n",
    "    url = result.get_attribute(\"href\")\n",
    "    results_data.append({\"Title\": title, \"URL\": url})\n",
    "\n",
    "# Crea un DataFrame con los resultados\n",
    "df = pd.DataFrame(results_data)\n",
    "\n",
    "# Imprime el DataFrame\n",
    "print(df)\n",
    "\n",
    "# Cierra el navegador\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del Script:\n",
    "    \n",
    "Para ejecutar el script, asegúrate de que los contenedores de Docker estén corriendo y luego ejecuta el siguiente comando en la terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose run --rm python python script.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTREGABLES: \n",
    "* script.py: El archivo con el código para realizar el web scraping y extraer los títulos de las noticias.\n",
    "* README.md: Instrucciones para ejecutar el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
