{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: Web Scraping con Selenium y Docker\n",
    "### Objetivo:\n",
    "\n",
    "Aprender a utilizar Selenium junto con Docker para realizar web scraping y extraer información de una página web. En esta tarea, extraerás los títulos de noticias de una página web de tu elección o la proporcionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de Web Scraping\n",
    "\n",
    "A continuación, te proporciono un script en Python utilizando Selenium para obtener los títulos de noticias de una página web. En este caso, utilizaremos la página de noticias de \"https://news.ycombinator.com/\" como ejemplo.\n",
    "\n",
    "Tu debes completar el script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecutar en modo sin cabeza\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de Hacker News\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "    \n",
    "    # Espera a que los elementos de los títulos estén presentes\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".titlelink\"))\n",
    "    )\n",
    "    \n",
    "    # Extrae los títulos de las noticias\n",
    "    titles = driver.find_elements(By.CSS_SELECTOR, \".titlelink\")\n",
    "    \n",
    "    # Imprime los títulos\n",
    "    print(\"Títulos de noticias en Hacker News:\")\n",
    "    for index, title in enumerate(titles, 1):\n",
    "        print(f\"{index}. {title.text}\")\n",
    "    \n",
    "    # Imprime el número total de títulos encontrados\n",
    "    print(f\"\\nTotal de títulos encontrados: {len(titles)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Cierra el navegador\n",
    "    driver.quit()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del Código:\n",
    "\n",
    "* Configuración de Selenium: Se configura Selenium con el navegador Chrome utilizando opciones específicas para ejecutarlo dentro de Docker.\n",
    "\n",
    "* Acceso a la Página Web: El script se conecta a la página de noticias de \"YCombinator\" y carga el contenido.\n",
    "\n",
    "* Extracción de Datos: Se utiliza find_elements con la clase storylink para obtener todos los títulos de las noticias en la página.\n",
    "\n",
    "* Impresión de Resultados: Los títulos extraídos se imprimen en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "* Buscar dentro de la página web alguno de los links de arriba (New, Past, etc) y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".topbar a\"))\n",
    "    )\n",
    "    \n",
    "    nav_links = driver.find_elements(By.CSS_SELECTOR, \".topbar a\")\n",
    "    \n",
    "    link_texts = []\n",
    "    link_hrefs = []\n",
    "    \n",
    "    for link in nav_links:\n",
    "        link_texts.append(link.text)\n",
    "        link_hrefs.append(link.get_attribute('href'))\n",
    "    \n",
    "    df_links = pd.DataFrame({\n",
    "        'Link Text': link_texts,\n",
    "        'Link URL': link_hrefs\n",
    "    })\n",
    "    \n",
    "    print(\"Links de navegación encontrados:\")\n",
    "    print(df_links)\n",
    "    \n",
    "    df_links.to_csv('hacker_news_links.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3:\n",
    "\n",
    "* Buscar algo dentro la pagina en el apartado de \"Search\" y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "    \n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "    )\n",
    "    \n",
    "    search_term = \"Python\"\n",
    "    \n",
    "    search_input.send_keys(search_term)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".storylink\"))\n",
    "    )\n",
    "    \n",
    "    search_results = driver.find_elements(By.CSS_SELECTOR, \".storylink\")\n",
    "    \n",
    "    titles = []\n",
    "    links = []\n",
    "    subtexts = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        titles.append(result.text)\n",
    "        links.append(result.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        subtext_elements = driver.find_elements(By.CSS_SELECTOR, \".subtext\")\n",
    "        for subtext in subtext_elements:\n",
    "            subtexts.append(subtext.text)\n",
    "    except:\n",
    "        subtexts = [\"No subtext available\"] * len(titles)\n",
    "    \n",
    "    df_search_results = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Link': links,\n",
    "        'Subtext': subtexts[:len(titles)] \n",
    "    })\n",
    "    \n",
    "    print(f\"Resultados de búsqueda para '{search_term}':\")\n",
    "    print(df_search_results)\n",
    "    \n",
    "    df_search_results.to_csv(f'hacker_news_search_{search_term}.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del Script:\n",
    "    \n",
    "Para ejecutar el script, asegúrate de que los contenedores de Docker estén corriendo y luego ejecuta el siguiente comando en la terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose run --rm python python script.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTREGABLES: \n",
    "* script.py: El archivo con el código para realizar el web scraping y extraer los títulos de las noticias.\n",
    "* README.md: Instrucciones para ejecutar el proyecto."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
