{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: Web Scraping con Selenium y Docker\n",
    "### Objetivo:\n",
    "\n",
    "Aprender a utilizar Selenium junto con Docker para realizar web scraping y extraer información de una página web. En esta tarea, extraerás los títulos de noticias de una página web de tu elección o la proporcionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de Web Scraping\n",
    "\n",
    "A continuación, te proporciono un script en Python utilizando Selenium para obtener los títulos de noticias de una página web. En este caso, utilizaremos la página de noticias de \"https://news.ycombinator.com/\" como ejemplo.\n",
    "\n",
    "Tu debes completar el script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conexión al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de noticias\n",
    "    driver.get(\"https://news.ycombinator.com/\")  # Página de ejemplo\n",
    "\n",
    "    # Extrae los títulos de las noticias\n",
    "    titles = driver.find_elements(By.CLASS_NAME, \"storylink\")\n",
    "    news_titles = [title.text for title in titles]\n",
    "\n",
    "    # Imprime los títulos\n",
    "    print(\"Títulos de noticias:\")\n",
    "    for title in news_titles:\n",
    "        print(title)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del Código:\n",
    "\n",
    "* Configuración de Selenium: Se configura Selenium con el navegador Chrome utilizando opciones específicas para ejecutarlo dentro de Docker.\n",
    "\n",
    "* Acceso a la Página Web: El script se conecta a la página de noticias de \"YCombinator\" y carga el contenido.\n",
    "\n",
    "* Extracción de Datos: Se utiliza find_elements con la clase storylink para obtener todos los títulos de las noticias en la página.\n",
    "\n",
    "* Impresión de Resultados: Los títulos extraídos se imprimen en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "* Buscar dentro de la página web alguno de los links de arriba (New, Past, etc) y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conexión al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de noticias\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "\n",
    "    # Extrae los enlaces del menú de navegación\n",
    "    navbar_links = driver.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "    links_data = [{\"Text\": link.text, \"URL\": link.get_attribute(\"href\")} for link in navbar_links]\n",
    "\n",
    "    # Convierte a un DataFrame y guarda en un archivo CSV\n",
    "    links_df = pd.DataFrame(links_data)\n",
    "    print(\"\\nLinks extraídos:\")\n",
    "    print(links_df)\n",
    "\n",
    "    links_df.to_csv(\"navbar_links.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3:\n",
    "\n",
    "* Buscar algo dentro la pagina en el apartado de \"Search\" y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conexión al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de noticias\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "\n",
    "    # Busca el campo de búsqueda y realiza una consulta\n",
    "    search_box = driver.find_element(By.NAME, \"q\")  # Asume que 'q' es el nombre del campo de búsqueda\n",
    "    search_query = \"Python\"\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Extrae los resultados de la búsqueda\n",
    "    search_results = driver.find_elements(By.CLASS_NAME, \"storylink\")\n",
    "    search_data = [{\"Title\": result.text, \"URL\": result.get_attribute(\"href\")} for result in search_results]\n",
    "\n",
    "    # Convierte a un DataFrame y guarda en un archivo CSV\n",
    "    search_df = pd.DataFrame(search_data)\n",
    "    print(\"\\nResultados de búsqueda:\")\n",
    "    print(search_df)\n",
    "\n",
    "    search_df.to_csv(\"search_results.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Codigo completo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración de las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Conexión al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Ejercicio 1: Extraer títulos de noticias\n",
    "    driver.get(\"https://news.ycombinator.com/\")  # Página de noticias\n",
    "    titles = driver.find_elements(By.CLASS_NAME, \"storylink\")\n",
    "\n",
    "    news_titles = [title.text for title in titles]\n",
    "    print(\"Títulos de noticias:\")\n",
    "    for title in news_titles:\n",
    "        print(title)\n",
    "\n",
    "    # Ejercicio 2: Extraer los links (New, Past, etc.)\n",
    "    navbar_links = driver.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "    links_data = [{\"Text\": link.text, \"URL\": link.get_attribute(\"href\")} for link in navbar_links]\n",
    "\n",
    "    links_df = pd.DataFrame(links_data)\n",
    "    print(\"\\nLinks extraídos:\")\n",
    "    print(links_df)\n",
    "\n",
    "    # Ejercicio 3: Buscar algo en el apartado \"Search\"\n",
    "    search_box = driver.find_element(By.NAME, \"q\")  # Asumiendo que 'q' es el nombre del input de búsqueda\n",
    "    search_query = \"Python\"\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Recopilar resultados de búsqueda\n",
    "    search_results = driver.find_elements(By.CLASS_NAME, \"storylink\")\n",
    "    search_data = [{\"Title\": result.text, \"URL\": result.get_attribute(\"href\")} for result in search_results]\n",
    "\n",
    "    search_df = pd.DataFrame(search_data)\n",
    "    print(\"\\nResultados de búsqueda:\")\n",
    "    print(search_df)\n",
    "\n",
    "    # Guardar los datos en CSV\n",
    "    links_df.to_csv(\"navbar_links.csv\", index=False)\n",
    "    search_df.to_csv(\"search_results.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Cerrar el navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del Script:\n",
    "    \n",
    "Para ejecutar el script, asegúrate de que los contenedores de Docker estén corriendo y luego ejecuta el siguiente comando en la terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose run --rm python python script.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTREGABLES: \n",
    "* script.py: El archivo con el código para realizar el web scraping y extraer los títulos de las noticias.\n",
    "* README.md: Instrucciones para ejecutar el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
