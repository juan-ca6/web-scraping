{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: Web Scraping con Selenium y Docker\n",
    "### Objetivo:\n",
    "\n",
    "Aprender a utilizar Selenium junto con Docker para realizar web scraping y extraer información de una página web. En esta tarea, extraerás los títulos de noticias de una página web de tu elección o la proporcionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de Web Scraping\n",
    "\n",
    "A continuación, te proporciono un script en Python utilizando Selenium para obtener los títulos de noticias de una página web. En este caso, utilizaremos la página de noticias de \"https://news.ycombinator.com/\" como ejemplo.\n",
    "\n",
    "Tu debes completar el script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecutar en modo sin cabeza\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de Hacker News\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "    \n",
    "    # Espera a que los elementos de los títulos estén presentes\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".titlelink\"))\n",
    "    )\n",
    "    \n",
    "    # Extrae los títulos de las noticias\n",
    "    titles = driver.find_elements(By.CSS_SELECTOR, \".titlelink\")\n",
    "    \n",
    "    # Imprime los títulos\n",
    "    print(\"Títulos de noticias en Hacker News:\")\n",
    "    for index, title in enumerate(titles, 1):\n",
    "        print(f\"{index}. {title.text}\")\n",
    "    \n",
    "    # Imprime el número total de títulos encontrados\n",
    "    print(f\"\\nTotal de títulos encontrados: {len(titles)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Cierra el navegador\n",
    "    driver.quit()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del Código:\n",
    "\n",
    "* Configuración de Selenium: Se configura Selenium con el navegador Chrome utilizando opciones específicas para ejecutarlo dentro de Docker.\n",
    "\n",
    "* Acceso a la Página Web: El script se conecta a la página de noticias de \"YCombinator\" y carga el contenido.\n",
    "\n",
    "* Extracción de Datos: Se utiliza find_elements con la clase storylink para obtener todos los títulos de las noticias en la página.\n",
    "\n",
    "* Impresión de Resultados: Los títulos extraídos se imprimen en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2:\n",
    "\n",
    "* Buscar dentro de la página web alguno de los links de arriba (New, Past, etc) y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecutar en modo sin cabeza\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de Hacker News\n",
    "    driver.get(\"https://news.ycombinator.com/\")\n",
    "    \n",
    "    # --- Extraer títulos de las noticias ---\n",
    "    # Espera a que los elementos de los títulos estén presentes\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".titlelink\"))\n",
    "    )\n",
    "    \n",
    "    # Extrae los títulos de las noticias\n",
    "    titles = driver.find_elements(By.CSS_SELECTOR, \".titlelink\")\n",
    "    \n",
    "    print(\"Títulos de noticias en Hacker News:\")\n",
    "    for index, title in enumerate(titles, 1):\n",
    "        print(f\"{index}. {title.text}\")\n",
    "    \n",
    "    print(f\"\\nTotal de títulos encontrados: {len(titles)}\\n\")\n",
    "    \n",
    "    # --- Extraer links de navegación ---\n",
    "    # Espera a que los elementos de navegación estén presentes\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".topbar a\"))\n",
    "    )\n",
    "    \n",
    "    # Encuentra todos los links de navegación\n",
    "    nav_links = driver.find_elements(By.CSS_SELECTOR, \".topbar a\")\n",
    "    \n",
    "    # Crear listas para almacenar datos de los links\n",
    "    link_texts = []\n",
    "    link_hrefs = []\n",
    "    \n",
    "    for link in nav_links:\n",
    "        link_texts.append(link.text)\n",
    "        link_hrefs.append(link.get_attribute('href'))\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df_links = pd.DataFrame({\n",
    "        'Link Text': link_texts,\n",
    "        'Link URL': link_hrefs\n",
    "    })\n",
    "    \n",
    "    print(\"Links de navegación encontrados:\")\n",
    "    print(df_links)\n",
    "    \n",
    "    # Guardar el DataFrame en un archivo CSV\n",
    "    df_links.to_csv('hacker_news_links.csv', index=False)\n",
    "    print(\"\\nLinks guardados en el archivo 'hacker_news_links.csv'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Cierra el navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3:\n",
    "\n",
    "* Buscar algo dentro la pagina en el apartado de \"Search\" y ponerlo en un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Configura las opciones de Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecutar en modo sin cabeza\n",
    "\n",
    "# Conéctate al servidor Selenium Grid\n",
    "driver = webdriver.Remote(\n",
    "    command_executor=\"http://selenium-server:4444/wd/hub\",\n",
    "    options=chrome_options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Abre la página de Hacker News\n",
    "    driver.get(\"https://hn.algolia.com/\")  # Página con función de búsqueda\n",
    "    \n",
    "    # Encuentra el campo de búsqueda\n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"input[placeholder='Search stories by title, url, or author']\"))\n",
    "    )\n",
    "    \n",
    "    # Término de búsqueda (puedes modificar esto)\n",
    "    search_term = \"Python\"\n",
    "    \n",
    "    # Introduce el término de búsqueda\n",
    "    search_input.send_keys(search_term)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Espera a que se carguen los resultados\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".item-title\"))\n",
    "    )\n",
    "    \n",
    "    # Extrae los resultados de búsqueda\n",
    "    search_results = driver.find_elements(By.CSS_SELECTOR, \".item-title\")\n",
    "    result_links = driver.find_elements(By.CSS_SELECTOR, \".item-title a\")\n",
    "\n",
    "    # Listas para almacenar datos de los resultados\n",
    "    titles = []\n",
    "    links = []\n",
    "    \n",
    "    # Extrae información de cada resultado\n",
    "    for result, link in zip(search_results, result_links):\n",
    "        titles.append(result.text)\n",
    "        links.append(link.get_attribute('href'))\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df_search_results = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Link': links,\n",
    "    })\n",
    "    \n",
    "    # Imprimir el DataFrame\n",
    "    print(f\"Resultados de búsqueda para '{search_term}':\")\n",
    "    print(df_search_results)\n",
    "    \n",
    "    # Guardar el DataFrame en un archivo CSV\n",
    "    filename = f'hacker_news_search_{search_term}.csv'\n",
    "    df_search_results.to_csv(filename, index=False)\n",
    "    print(f\"Resultados guardados en el archivo '{filename}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Cierra el navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del Script:\n",
    "    \n",
    "Para ejecutar el script, asegúrate de que los contenedores de Docker estén corriendo y luego ejecuta el siguiente comando en la terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose run --rm python python script.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTREGABLES: \n",
    "* script.py: El archivo con el código para realizar el web scraping y extraer los títulos de las noticias.\n",
    "* README.md: Instrucciones para ejecutar el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
